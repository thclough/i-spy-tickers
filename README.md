This model identifies where public companies are mentioned in a body of text: both plain company names, nicknames, and tickers. Detection of company names is not case sensitive.

For data collection, I first amalgamated a list of public companies/securities from various sources then crawled a popular financial news site for over 30,000 articles on these public companies. The majority of the time this news site mentions public companies in their articles, they include a ticker symbol in parentheses with a link after the first mention of the company. While not easy, this meant I could parse the articles for these tickers, check if a certain form of the company name came before the ticker, and mark these preceding words as positive examples of a company mentioned. The labeling algorithm also gathers on a per-article basis the references to these companies (ex. Eastman Kodak referred to as Kodak), and searches the rest of the article for any matching references (to gather more positive examples). Data was cleaned with a mind-numbing amount or regex at first, but then scaled back for more simplicity.

I ran a preliminary data analysis with a simple (self-implemented from scratch) Naive Bayes algorithm to see if words within a certain sized context window of a target word could indicate if the target word was part of a public company mention or not. (no embeddings used for this, straight sparse binary vectors). Promising results prompted further development of a more complex model.

# BiLSTM

Embedding creation located [here](https://colab.research.google.com/drive/1st9r-ArxYlg7qqCP7EAGgoa3m5rf8_65?usp=sharing)
BiLSTM Colab notebook located [here](https://colab.research.google.com/drive/1wBymA9Myl9eRoCohmluRa3Jz4syEOMU9)
Weights uploaded to hugging face [here](https://huggingface.co/thclough/find_mentioned_companies) (make sure to load model in an environment that includes the registered tail_train_embedding layer located in the BiLSTM notebook)

I created a bidirectional LSTM in Tensorflow Keras. I created a custom data loader using tf.data and generator functions given the large size of the train set. I based my text cleaning around the tokens word2vec Google News embeddings, which do not include embeddings for common stop words (as is practice). I chose word2vec over Glove vectors because training new word2vec embeddings for stop words was much easier than training new GloVe vectors. I chose to include stop words because I believe they are helpful in determining if a surrounding word is a company (the apple fell vs. Apple fell). Determiners ("the", "an") help to discriminate common vs proper nouns. I created a custom embedding layer (TailTrainEmbedding) to freeze embeddings for existing word2vec tokens while allowing for the training of stop word embeddings during LSTM training. I tested the custom embedding layer against a simple frozen embedding (no learning for stop words, using random normal initialized embeddings for stop words, similar to UNK token). The experiment indicated custom embedding with stop words outperformed a simple frozen embedding on the dev set (.816 vs .770 after 5 epochs). The inclusion of stop words provided a reasonable boost in performance. One example corresponds to one sentence. Irregular sentence lengths were accommodated with padding and masking. I used a custom weighted binary cross entropy loss to make up for the biased dataset (small proportion of positive examples).

No extensive hyperparameter tuning was performed except for experimentation with state size (a shift from 32 to 64 units) in the LSTM.

The best model achieved results with an F1 score .830 during evaluation on test set. The model's weakness is false positives on acronyms like "AI" (common acronyms that are also tickers). Error analysis on a sampled section of the eval/dev set indicated the most common error (around half of all errors) was actually that an insignificant amount of positive examples were _mislabeled_ as negative meaning the model learned past the imperfections in the labeling algorithm. This indicated the true value of the F1 metric could be much higher.
